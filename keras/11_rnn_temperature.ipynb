{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is in folder data/jena_data/jena_climate_2009_2016.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "datafile = \"../data/jena_data/jena_climate_2009_2016.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (420551, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2009 00:10:00</td>\n",
       "      <td>996.52</td>\n",
       "      <td>-8.02</td>\n",
       "      <td>265.40</td>\n",
       "      <td>-8.90</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1307.75</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.75</td>\n",
       "      <td>152.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2009 00:20:00</td>\n",
       "      <td>996.57</td>\n",
       "      <td>-8.41</td>\n",
       "      <td>265.01</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>93.4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1309.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.50</td>\n",
       "      <td>136.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2009 00:30:00</td>\n",
       "      <td>996.53</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>264.91</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>93.9</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1310.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>171.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.01.2009 00:40:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.31</td>\n",
       "      <td>265.12</td>\n",
       "      <td>-9.07</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1309.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2009 00:50:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>265.15</td>\n",
       "      <td>-9.04</td>\n",
       "      <td>94.1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
       "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
       "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
       "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
       "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
       "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
       "\n",
       "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "0          3.33          3.11          0.22       1.94             3.12   \n",
       "1          3.23          3.02          0.21       1.89             3.03   \n",
       "2          3.21          3.01          0.20       1.88             3.02   \n",
       "3          3.26          3.07          0.19       1.92             3.08   \n",
       "4          3.27          3.08          0.19       1.92             3.09   \n",
       "\n",
       "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0       1307.75      1.03           1.75     152.3  \n",
       "1       1309.80      0.72           1.50     136.1  \n",
       "2       1310.24      0.19           0.63     171.6  \n",
       "3       1309.19      0.34           0.50     198.0  \n",
       "4       1309.00      0.32           0.63     214.3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"data shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Date Time\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the temperature column as target data, convert it to numpy array\n",
    "targets = df['T (degC)'].values\n",
    "# convert the dataframe into a numpy matrix called `data`\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains measurements done each 10 minutes, so there are 144 measurements in a day. We can define the following parameters of the model:\n",
    "- lookback = 1440 - we'll take data of last 10 days as direct input to the model\n",
    "- steps = 6 - we won't use all data points since many features don't change much in less then an hour. We'll sample data using steps of 1 hour \n",
    "- delay = 144 - how further in the future we'd like to predict (24 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "steps = 6\n",
    "delay = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 200000\n",
    "val_samples = 100000\n",
    "# test samples would take the rest of data (or 120551 data rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize and standardize the data - subtract the mean and divide with the standard deviation of data. Here, the mean and st. dev are calculated only on the training set, but applied to the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90014748 -1.93135845 -1.98211036 -1.86280029  1.07285236 -1.30742164\n",
      "  -1.47375773 -0.79868641 -1.4762674  -1.47815522  2.12375056 -0.72950452\n",
      "  -0.78067973 -0.27613603]\n",
      " [ 0.9060434  -1.97541381 -2.02567    -1.91582958  1.07883061 -1.32042698\n",
      "  -1.4951961  -0.80075238 -1.49502455 -1.49932141  2.17199852 -0.93124017\n",
      "  -0.88794488 -0.46317443]]\n"
     ]
    }
   ],
   "source": [
    "mean = data[:train_samples].mean(axis=0)\n",
    "data -= mean\n",
    "std = data[:train_samples].std(axis=0)\n",
    "data /= std\n",
    "\n",
    "# print 2 lines of the processed data\n",
    "print(data[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420551, 14)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a Python generator function that will take as input our data and output batches of data, in the suitable format (batch_size, timesteps, num_features) to be inserted in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\n",
    "        else:\n",
    "            # make sure you don't go over the length of data\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        \n",
    "        # output format: (batch_size, timesteps, num_features)\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        # loop over a batch and create samples and targets\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above generator function can now be used to generate the train, validation and test dataset generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=0,\n",
    "                     max_index=train_samples,\n",
    "                     shuffle=True,\n",
    "                     step=steps,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=train_samples + 1,\n",
    "                    max_index=train_samples + val_samples,\n",
    "                    shuffle=False,\n",
    "                    step=steps,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=train_samples + val_samples + 1,\n",
    "                    max_index=None,\n",
    "                    shuffle=False,\n",
    "                    step=steps,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "# Number of steps to see the entire validation set\n",
    "val_steps = train_samples + val_samples - train_samples - lookback - 1\n",
    "# Number of steps to see the entire test set\n",
    "test_steps = len(data) - train_samples - val_samples - lookback - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28973058236861327\n"
     ]
    }
   ],
   "source": [
    "def evaluate2():\n",
    "    batch_maes = []\n",
    "    for samples, targets in val_gen:\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 805s 2s/step - loss: 1.3603 - val_loss: 0.5123\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 803s 2s/step - loss: 0.4277 - val_loss: 0.3434\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 796s 2s/step - loss: 0.2948 - val_loss: 0.3504\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 790s 2s/step - loss: 0.2674 - val_loss: 0.2982\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 797s 2s/step - loss: 0.2520 - val_loss: 0.3160\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 791s 2s/step - loss: 0.2410 - val_loss: 0.3163\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 785s 2s/step - loss: 0.2358 - val_loss: 0.3224\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 777s 2s/step - loss: 0.2302 - val_loss: 0.3190\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 782s 2s/step - loss: 0.2249 - val_loss: 0.3092\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 792s 2s/step - loss: 0.2217 - val_loss: 0.3190\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 793s 2s/step - loss: 0.2180 - val_loss: 0.3293\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 787s 2s/step - loss: 0.2150 - val_loss: 0.3308\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 787s 2s/step - loss: 0.2114 - val_loss: 0.3132\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 797s 2s/step - loss: 0.2107 - val_loss: 0.3150\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 795s 2s/step - loss: 0.2087 - val_loss: 0.3241\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 788s 2s/step - loss: 0.2059 - val_loss: 0.3272\n",
      "Epoch 17/20\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2033"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // steps, data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                             steps_per_epoch = 500,\n",
    "                             epochs=20,\n",
    "                             validation_data=val_gen,\n",
    "                             validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots the history of a model training - its loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
